load("~/Data/Save 05_11.RData")
library(tidyverse)
library(psych) # For alpha and omega
library(mice) # To impute missing grades
library(mirt) # For marginal reliability
library(MBESS) # For the GLB
library(Rcsdp) # Additional requirement for the GLB
# Multiple imputation way of dealing with optionality ----
reliabilities_imp <- lapply(components, function(component) {
print(component[1, 1])  # Print first element for tracking
# Step 1: Select relevant columns and remove columns with completely missing data
items <- component %>%
dplyr::select(-cand_anon, -NO_LANG_CODE) %>%
select_if(~ !all(is.na(.)))
# Step 2: Remove duplicate columns that are perfectly correlated
cor_matrix <- cor(items, use = "pairwise.complete.obs")
# Find perfectly correlated pairs (correlation == 1 or -1)
perfectly_correlated <- which(abs(cor_matrix) == 1 & lower.tri(cor_matrix), arr.ind = TRUE)
# Get unique column indices of perfectly correlated items to remove
if (nrow(perfectly_correlated) > 0) {
items_to_remove <- unique(perfectly_correlated[, 2])
# Remove the redundant columns from items
items <- items[, -items_to_remove]
}
# Apply this only where possible (MI will struggle if there are not enough observations)
if (nrow(items) > ncol(items) & component[1, 1] %in% optionality_names) {
# Step 3: Detect rows where students answered only a subset of items
is_sparse_answering <- function(df, threshold = 0.5) {
n_cols <- ncol(df)
sparse_rows <- apply(df, 1, function(row) {
answered_count <- sum(row != 0)
# If fewer than 'threshold' percentage of columns are answered, mark it
return(answered_count < threshold * n_cols)
})
return(sparse_rows)
}
# Apply the sparse pattern detection with a threshold (e.g., 50% answered)
sparse_rows <- is_sparse_answering(items, threshold = 0.5)
# Check if more than 1/3 of participants have sparse rows
if (mean(sparse_rows) > 1/3) {
# Step 4: For the detected sparse rows, replace 0s with NA (they are considered missing)
items[sparse_rows, ][items[sparse_rows, ] == 0] <- NA
}
}
# Step 5: Remove columns with zero variance
item_variances <- sapply(items, var, na.rm = TRUE)
items_with_variance <- items[, !is.na(item_variances) & item_variances != 0]
# Check if there are enough rows and columns for Cronbach's alpha
if (nrow(items) > 1 & ncol(items_with_variance) > 1) {
# Step 6: Perform multiple imputation on items_with_variance
if (any(is.na(items_with_variance))) {
# Rename columns
oldcols <- colnames(items_with_variance)
colnames(items_with_variance) <- paste0('col', 1:ncol(items_with_variance))
# Perform multiple imputation with 5 imputations
imputed_data <- mice(items_with_variance, m = 5, method = 'pmm', maxit = 50, seed = 500, printFlag = FALSE)
# Step 7: Calculate Cronbach's alpha for each imputed dataset and average
alpha_values <- sapply(1:5, function(i) {
# Get the ith imputed dataset
completed_items <- complete(imputed_data, i)
# Calculate max value for Cronbach's alpha
max_value <- max(completed_items, na.rm = TRUE)
# Compute Cronbach's alpha for the i-th imputed dataset
return(alpha(completed_items, max = max_value + 1, warnings = FALSE, na.rm = TRUE)$total$raw_alpha)
})
# Bring back the old column names after imputation
colnames(items_with_variance) <- oldcols
# Return the average of the alpha values across all imputations and imputation status
return(list(reliability = mean(alpha_values, na.rm = TRUE), imputation = "imp"))
} else {
# If no missing data, use the original data
completed_items <- items_with_variance
max_value <- max(completed_items, na.rm = TRUE)
return(list(reliability = alpha(completed_items, max = max_value + 1, warnings = FALSE, na.rm = TRUE)$total$raw_alpha, imputation = "no imp"))
}
} else {
return(list(reliability = NA, imputation = NA))  # Return NA if not enough rows or columns to calculate reliability
}
})
# Multiple imputation way of dealing with optionality ----
reliabilities_imp <- lapply(components, function(component) {
print(component[1, 1])  # Print first element for tracking
# Step 1: Select relevant columns and remove columns with completely missing data
items <- component %>%
dplyr::select(-cand_anon, -NO_LANG_CODE) %>%
select_if(~ !all(is.na(.)))
# Step 2: Remove duplicate columns that are perfectly correlated
cor_matrix <- cor(items, use = "pairwise.complete.obs")
# Find perfectly correlated pairs (correlation == 1 or -1)
perfectly_correlated <- which(abs(cor_matrix) == 1 & lower.tri(cor_matrix), arr.ind = TRUE)
# Get unique column indices of perfectly correlated items to remove
if (nrow(perfectly_correlated) > 0) {
items_to_remove <- unique(perfectly_correlated[, 2])
# Remove the redundant columns from items
items <- items[, -items_to_remove]
}
# Apply this only where possible (MI will struggle if there are not enough observations)
if (nrow(items) > ncol(items) & component[1, 1] %in% optionality_names) {
# Step 3: Detect rows where students answered only a subset of items
is_sparse_answering <- function(df, threshold = 0.5) {
n_cols <- ncol(df)
sparse_rows <- apply(df, 1, function(row) {
answered_count <- sum(row != 0)
# If fewer than 'threshold' percentage of columns are answered, mark it
return(answered_count < threshold * n_cols)
})
return(sparse_rows)
}
# Apply the sparse pattern detection with a threshold (e.g., 50% answered)
sparse_rows <- is_sparse_answering(items, threshold = 0.5)
# Check if more than 1/3 of participants have sparse rows
if (mean(sparse_rows) > 1/3) {
# Step 4: For the detected sparse rows, replace 0s with NA (they are considered missing)
items[sparse_rows, ][items[sparse_rows, ] == 0] <- NA
}
}
# Step 5: Remove columns with zero variance
item_variances <- sapply(items, var, na.rm = TRUE)
items_with_variance <- items[, !is.na(item_variances) & item_variances != 0]
# Check if there are enough rows and columns for Cronbach's alpha
if (nrow(items) > 1 & ncol(items_with_variance) > 1) {
# Step 6: Perform multiple imputation on items_with_variance
if (any(is.na(items_with_variance))) {
# Rename columns
oldcols <- colnames(items_with_variance)
colnames(items_with_variance) <- paste0('col', 1:ncol(items_with_variance))
# Perform multiple imputation with 5 imputations
imputed_data <- mice(items_with_variance, m = 5, method = 'pmm', maxit = 50, seed = 500, printFlag = FALSE)
# Step 7: Calculate Cronbach's alpha for each imputed dataset and average
alpha_values <- sapply(1:5, function(i) {
# Get the ith imputed dataset
completed_items <- complete(imputed_data, i)
# Calculate max value for Cronbach's alpha
max_value <- max(completed_items, na.rm = TRUE)
# Compute Cronbach's alpha for the i-th imputed dataset
return(alpha(completed_items, max = max_value + 1, warnings = FALSE, na.rm = TRUE)$total$raw_alpha)
})
# Bring back the old column names after imputation
colnames(items_with_variance) <- oldcols
# Return the average of the alpha values across all imputations and imputation status
return(list(reliability = mean(alpha_values, na.rm = TRUE), imputation = "imp"))
} else {
# If no missing data, use the original data
completed_items <- items_with_variance
max_value <- max(completed_items, na.rm = TRUE)
return(list(reliability = alpha(completed_items, max = max_value + 1, warnings = FALSE, na.rm = TRUE)$total$raw_alpha, imputation = "no imp"))
}
} else {
return(list(reliability = NA, imputation = NA))  # Return NA if not enough rows or columns to calculate reliability
}
})
which(components == "arthsSP1TZ0XXXX")
which(names(components) == "arthsSP1TZ0XXXX")
comp_trunc <- components[-(1:252)]
library(Hmisc)
library(mice)
library(tidyverse)
renv::status()
renv::install(c("Hmisc", "mice", "tidyverse"))
library(Hmisc)
library(mice)
library(tidyverse)
# Remove problematic packages from the cache
renv::purge("rlang")
# Reinstall them
renv::install(c("rlang", "Rcpp", "magrittr"))
.libPaths()
library(Hmisc)
library(mice)
library(tidyverse)
library(Hmisc)
library(Hmisc)
library(Hmisc)
renv::init()
install.packages('renv')
renv::init()
renv::install(c("cluster", "foreign"))
renv::install("knitr")
renv::snapshot()
library(Hmisc)
library(mice)
library(tidyverse)
renv::install("magrittr")
library(tidyverse)
.libPaths()
library(Hmisc)
library(mice)
library(tidyverse)
renv::install("rlang", type = "source")
renv::install("Rcpp", type = "source")
renv::install("magrittr", type = "source")
library(tidyverse)
renv::install("magrittr", type = "source")
library(tidyverse)
R.version.string
Sys.which("make")
renv::deactivate()
install.packages(c("Hmisc", "mice", "tidyverse"))
library(Hmisc)
library(mice)
library(tidyverse)
tempdir <- tempfile()
dir.create(tempdir)
.libPaths(tempdir)
install.packages(c("rlang", "Rcpp", "magrittr", "Hmisc", "mice", "tidyverse"))
renv::rebuild()
install.packages('renv')
renv::init()
renv::rebuild()
library(Hmisc)
library(mice)
library(tidyverse)
library(Hmisc)
library(mice)
library(tidyverse)
.libPaths()
renv::settings$use.cache(FALSE)
renv::restore()
library(Hmisc)
library(mice)
library(tidyverse)
renv::settings$use.cache()
install.packages('renv')
renv::init()
library(Hmisc)
library(Hmisc)
install.packages('HMisc')
install.packages('mice')
library(mice)
installed.packages('tidyverse')
install.packages('tidyverse')
library(tidyverse)
install.packages('renv')
renv::init()
install.packages('Hmisc')
library(Hmisc)
library(mice)
install.packages('mice')
library(mice)
library(tidyverse)
install.packages("REN"')
install.packages('renv')
renv::init()
renv::init()
renv::init()
install.packages('renv')
renv::init()
install.packages('Hmisc')
library(Hmisc)
install.packages('mice')
library(mice)
install.packages('rlang')
library(rlang)
find.package("rlang")
installed.packages()["rlang", ]
.libPaths()
R.home()
sessionInfo()
wd
wd()
setwd()
dir
dir()
renv::init()
renv::settings$use.cache(FALSE)
renv::install("rlang", type = "binary")
packageDescription("rlang")$Built
installed.packages()["rlang", ]
install.packages('Hmisc')
library(Hmisc)
update.packages('rlang')
installed.packages()["rlang", ]
remove.packages("rlang")
install.packages("rlang", type = "source")
installed.packages()["rlang", ]
install.packages("rlang", type = "source")
installed.packages()["rlang", ]
